---
layout:     post
title:      计算机基础
subtitle:   
date:       2022-12-6
author:     
header-img:
catalog: true
tags:
    - 计算机基础
---
## 计算机的简略发展史
### 电子管时代
背景：为了军方的计算要求，比如弹道轨迹计算，人们的需要一个能代替人脑的计算装置。

当时的计算机会有很多逻辑处理元件，它们在高低电压（可以表示01的二进制）下用线路连接起来实现计算的功能。

当时的主要逻辑元件是电子管，体积大，耗电量大。此时的计算机只能识别0101的二进制数，所以只能用机器语言来编程。

### 晶体管时代
背景：希望计算机体积、耗电量、计算能力等方面比上个时代更出色。

晶体管的电气特性可以替代电子管，而且晶体管的体积比电子管小很多，这也意味着此时的计算机要小很多。
并且出现了面向过程的程序设计语言和操作系统的雏形，制造一台计算机大概需要几万到几十万的晶体管，并且需要用手工的方式把晶体管焊接到电路板上，就非常容易出错。

### 中小规模集成电路时代
集成电路的技术让我们计算机变得越来越小，同时功耗更低，可靠性也比手动焊接的晶体管更高，
此时的计算机主要用于科学计算，一些高级语言同时产生，并出现分时操作系统

### 超大规模集成电路时代
随着集成电路工艺的不断提升，出现了大规模和超大规模集成电路，此时开始出现微处理和微型计算机，也就是我们现在家用的计算机，就拿苹果的A13处理器来说，每一个逻辑元件在其中不超过7纳米，一个指甲盖大小的cpu就集成了85亿个晶体管。

## 为什么计算机内部选择二进制数表示
- 技术实现简单，计算机是由逻辑电路组成，逻辑电路通常只有两个状态，开关的接通与断开，这两种状态正好可以用 “1” 和 “0” 表示;
- 易于进行转换，二进制与十进制数易于互相转换;
- 适合逻辑运算,逻辑代数是逻辑运算的理论依据，二进制只有两个数码，正好与逻辑代数中的“真”和“假”相吻合;

## 不同进制的转换
1、任意进制转十进制。
- 每位数字乘以对应进制的位数次方后相加，位从0开始。例如：2进制的 101.1 = 1 x 2的2次方 + 0 x 2的1次方 + 1 x 2的0次方 + 1 x 2的-1次方
- JS：toString(radix)

2、十进制整数转为任意进制。
- 除商取余法。除对应进制。将余数由后往前拼接得到结果。
- JS：parseInt(num, radix);小数部分被截断。

例如：把89化为二进制的数

89÷2=44 余1

44÷2=22 余0

22÷2=11 余0

11÷2=5 余1

5÷2=2 余1

2÷2=1 余0

1÷2=0 余1

结果：1011001

3、十进制小数转二进制小数
方式是采用“乘2取整，顺序排列”法。
1、用2乘十进制小数，可以得到积，将积的整数部分取出-
2、再用2乘余下的小数部分，又得到一个积，再将积的整数部分取出-
3、如此进行，直到积中的小数部分为零，或者达到所要求的精度为止

如: 十进制 0.25 转为二进制

0.25 * 2 = 0.5 取出整数部分：0

0.5 * 2 = 1.0 取出整数部分1

即十进制0.25的二进制为 0.01 ( 第一次所得到为最高位,最后一次得到为最低位)。

而0.1和0.2转换成二进制是无限循环的。所以0.1+0.2!==0.3

## 真值和机器数
例如：

+15 => 01111（2进制）

-8 => 11000（2进制）

真值是我们平时生活中用到的数字形式，比如+15，-8。机器数是存到机器里的形式，也就是2进制的形式，其中01111，第一个0是代表正数的意思，1111是保存的数值，转换成10进制就是15，合起来就是+15。11000同理。

## 单位
计算机内部，所有信息最终都是一个二进制值。每一个二进制位（bit）有0和1两种状态，因此八个二进制位就可以组合出256种状态，这被称为一个字节(byte)。

转换关系：

8位 = 1字节

1024字节 = 1K

1024K = 1M

1024M = 1G

1024G = 1T

## 编码
### ASCII
0-32种状态规定了特殊用途,接收到约定好的这些字节，就要做一些约定的动作。又把所有的空格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第 127 号，这样计算机就可以用不同字节来存储英语的文字了

这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为0。

### GB2312
西欧一些国家用的不是英文，它们的字母在ASCII里没有为了可以保存他们的文字，他们使用127号这后的空位来保存新的字母，一直编到了最后一位255。
从128 到 255 这一页的字符集被称为扩展字符集。在不同编码中表示的也不同。

中国为了表示汉字，把127号之后的符号取消了，规定：
一个小于127的字符的意义与原来相同，但两个大于 127 的字符连在一起时，就表示一个汉字；
前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从 0xA1 到 0xFE。
这样我们就可以组合出大约7000多个(247-161)*(254-161)=(7998)简体汉字了。
还把数学符号、日文假名和ASCII里原来就有的数字、标点和字母都重新编成两个字长的编码。这就是全角字符，127以下那些就叫半角字符。
把这种汉字方案叫做 GB2312。GB2312 是对 ASCII 的中文扩展
### GBK
不再要求低字节一定是 127 号之后的内码，只要第一个字节是大于 127 就固定表示这是一个汉字的开始,又增加了近 20000 个新的汉字（包括繁体字）和符号。
### Unicode
ISO 的国际组织废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符 的编码！ Unicode 当然是一个很大的集合，现在的规模可以容纳100多万个符号。
ISO 就直接规定必须用两个字节，也就是 16 位来统一表示所有的字符，对于 ASCII 里的那些 半角字符，Unicode 保持其原编码不变，只是将其长度由原来的 8 位扩展为16 位，而其他文化和语言的字符则全部重新统一编码。
从 Unicode 开始，无论是半角的英文字母，还是全角的汉字，它们都是统一的一个字符！同时，也都是统一的 两个字节
注意：字节是一个8位的物理存贮单元， 而字符则是一个文化相关的符号。
### 平面（Plane）
Unicode 使用的数字是从 0 到 0x10ffff，这些数字都对有相对应的字符（当然，有的还没有编好，有的用作私人自定义）。每一个数字，就是一个代码点（Code Point）。
这些代码点，分为 17 个平面（Plane）。其实就是17 组。
Plane 3 到 Plane 14 还没有使用。
Plane 0，习惯上称作基本平面（Basic Plane）；剩余的称作扩展平面（Supplementary Plane）。
### utf-32
UTF-32 使用四个字节来表示存储代码点：把代码点转换为 32 位二进制，位数不够的左边充 0。
4个字节就是4 * 8 = 32位，就能表示2的32次方个数字，这些数字可以对应2的32次方个字符
### utf-16
UTF-16 用二个字节来表示基本平面，用四个字节来表示扩展平面。也就是说，UTF-16的编码长度要么是2个字节（U+0000到U+FFFF），要么是4个字节（U+010000到U+10FFFF）
### utf-8
UTF-8是一种变长的编码方法，字符长度从1个字节到4个字节不等。 越是常用的字符，字节越短，最前面的128个字符，只使用1个字节表示，与ASCII码完全相同。

### 中文在unicode里面的范围
4E00~9FA5：中日韩统一表意文字 其余有需要自查。4E00－9FA5 就是一般正则表达式匹配中文的范围。
### 在javascript中，如何转utf8呢？
可以使用encodeURIComponent
平时我们说中文是两个字节表示的，这个是错误的，几个字节表示完全是看编码，比如utf8和utf16有可能同样的unicode码，编码出来的字节数是不一样的。 我们平时的页面都是utf8编码的，其实在底层2进制上，中文通常是3个字节表示的。
### JavaScript 如何在内部使用 Unicode
虽然 JavaScript 源文件可以有任何类型的编码，但 JavaScript 会在执行之前在内部将其转换为 UTF-16。
JavaScript 字符串都是 UTF-16 序列，正如 ECMAScript 标准所说： 当 String 包含实际文本数据时，每个元素都被视为单个 UTF-16 代码单元。
## 定点数
### 无符号数
就是整个机器字长的全部二进制位均为数值位，没有符号位，相当于都是正数。机器字长是指计算机进行一次整数运算所能处理的二进制数据的位数，比如我们常说32位机器，64位机器。
比如8位无符号整数的范围就是 二进制： 00000000 - 11111111，转化为10进制就是0 - 255。
注意我们说的无符号数都是针对整数，没有小数。

## 资料
[前端计算机基础大补丸在此！来一颗吧！](https://juejin.cn/post/7112318717798645768)
